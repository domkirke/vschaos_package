{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning and Supervision\n",
    "\n",
    "While variational auto-encoders can trained entirely in an unsupervised way, the obtained latent space is not constrained to any explicitely defined structure. Hence, without any further constraints, interaction of the limited is limited to a direct exploration of the model. To this end, metadata can be benefically used to enforce target structure to the model, allowing whether to shape the explored space, or to condition the generation in order to control the desired output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/conds.png\" style=\"width: 500px;\"/>\n",
    "<center> <b>(a)</b> unsupervised case, <b>(b)</b> conditioned case, <br/>\n",
    "<b>(c)</b> deterministic semi-supervised case, <b>(d)</b> variational semi-supervised case </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "If the metadata regarding a specific task is entirely available, a classification loss can be added to the model's loss using a discriminator taking the latent space as input. Provided the small capacity of the discriminator, this will force the latent space to be easilly separated by the classifier, enforcing data sharing the same label to be clustered together. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, vschaos\n",
    "import vschaos.distributions as dist\n",
    "import vschaos.vaes as vaes\n",
    "from vschaos.data import dataset_from_torchvision, Flatten\n",
    "\n",
    "from torchvision.transforms import Lambda, ToTensor\n",
    "from vschaos.vaes import VanillaVAE\n",
    "\n",
    "from vschaos.criterions import ELBO, Classification\n",
    "from vschaos.monitor import PCA\n",
    "from vschaos.train import SimpleTrainer, train_model\n",
    "\n",
    "# import MNIST\n",
    "transforms = [Lambda(lambda x: x / 255. + 0.01*torch.randn_like(x.float())), Flatten(-2)]\n",
    "dataset = dataset_from_torchvision('MNIST', transforms=transforms)\n",
    "\n",
    "# make VAE\n",
    "input_params = {'dim':784, 'dist': dist.Normal}\n",
    "class_params = {'dim':10, 'dist':dist.Categorical}\n",
    "hidden_params = {'dim':800, 'nlayers':2, 'batch_norm':'batch'}\n",
    "latent_params = {'dim':8, \"dist\":dist.Normal}\n",
    "vae = vaes.VanillaVAE(input_params, latent_params, hidden_params=hidden_params)\n",
    "\n",
    "# initialize optimizer\n",
    "optim_params = {'optimizer':'Adam', 'optimArgs':{'lr':1e-5}, 'scheduler':'ReduceLROnPlateau'}\n",
    "vae.init_optimizer(optim_params)\n",
    "\n",
    "# defining the losses\n",
    "elbo = ELBO(beta=1.0, warmup=20)\n",
    "classifier = Classification(latent_params, \"class\", class_params,\n",
    "                            layer=0, hidden_params={'dim':50, 'nlayers':2}, optim_params = {'lr':1e-5})\n",
    "loss = elbo + classifier\n",
    "\n",
    "# The Trainer object performs training, monitoring, and automating saving during the training process.\n",
    "plots = {}\n",
    "plots['reconstructions'] = {'preprocess': False, \"n_points\":15, 'plot_multihead':True, 'label':['class']}\n",
    "plots['latent_space'] = {'preprocess':False, 'transformation':PCA, 'tasks':'class', 'balanced':True, 'n_points':3000, 'label':['class'], 'batch_size':512}\n",
    "plots['confusion'] = {'classifiers':classifier}\n",
    "trainer = SimpleTrainer(vae, dataset, loss, tasks=[\"class\"], plots=plots, use_tensorboard=\"runs/\")\n",
    "\n",
    "train_options = {'epochs':100, 'save_epochs':20, 'results_folder':'tutorial_3',  'batch_size':64}\n",
    "train_model(trainer, train_options, save_with={'transforms':dataset.classes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning\n",
    "\n",
    "While adding classification losses to the ELBO may help to organize the latent space, it does not provide a manner of enforcing the target class of the generation. An efficient way to constrain the generative process $p(\\mathbf{x}|\\mathbf{z})$ is to _condition_ this distribution to the incoming label information, such that we rather model the distributions $p(\\mathbf{x}|\\mathbf{z, y})$ and/or $q(\\mathbf{z}|\\mathbf{x, y})$. Computationally, this can be done simply by concatenating the label information to the respective inputs of the decoder and/or the encoder. Conditioning the sole decoder may allow to have an inference process global to every class, while conditioning the encoder provides seperate latent spaces for every class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, vschaos\n",
    "import vschaos.distributions as dist\n",
    "import vschaos.vaes as vaes\n",
    "from vschaos.data.data_generic import dataset_from_torchvision\n",
    "\n",
    "from torchvision.transforms import Lambda, ToTensor\n",
    "from vschaos.vaes import VanillaVAE\n",
    "\n",
    "from vschaos.criterions import ELBO, Classification\n",
    "from vschaos.monitor import PCA\n",
    "from vschaos.train import SimpleTrainer, train_model\n",
    "\n",
    "# import MNIST\n",
    "transforms = [Lambda(lambda x: x / 255. + 0.01*torch.randn_like(x.float())), Flatten(-2)]\n",
    "dataset = dataset_from_torchvision('MNIST', transforms=transforms)\n",
    "\n",
    "# make VAE\n",
    "input_params = {'dim':784, 'dist': dist.Normal}\n",
    "class_params = {'dim':10, 'dist':dist.Categorical}\n",
    "hidden_params = {'encoder':{'dim':800, 'nlayers':2, 'batch_norm':'batch'}, \n",
    "                 'decoder':{'dim':800, 'nlayers':2, 'batch_norm':'batch',\n",
    "                            'label_params':{'class':class_params}}} # we condition by adding the label_params keyword\n",
    "latent_params = {'dim':8, \"dist\":dist.Normal}\n",
    "vae = vaes.VanillaVAE(input_params, latent_params, hidden_params=hidden_params)\n",
    "print(vae.decoders)\n",
    "\n",
    "optim_params = {'optimizer':'Adam', 'optimArgs':{'lr':1e-5}, 'scheduler':'ReduceLROnPlateau'}\n",
    "vae.init_optimizer(optim_params)\n",
    "elbo = ELBO(beta=1.0, warmup=20)\n",
    "\n",
    "# The Trainer object performs training, monitoring, and automating saving during the training process.\n",
    "plots = {}\n",
    "plots['reconstructions'] = {'preprocess': False, \"n_points\":15, 'plot_multihead':True, 'label':['class']}\n",
    "plots['latent_space'] = {'preprocess':False, 'transformation':PCA, 'tasks':'class', 'balanced':True, 'n_points':3000, 'label':['class'], 'batch_size':512}\n",
    "trainer = SimpleTrainer(vae, dataset, elbo, tasks=[\"class\"], plots=plots, use_tensorboard=\"runs/\")\n",
    "\n",
    "train_options = {'epochs':100, 'save_epochs':20, 'results_folder':'tutorial_3',  'batch_size':64}\n",
    "train_model(trainer, train_options, save_with={'transforms':dataset.classes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervision\n",
    "\n",
    "While conditioning the encoding / decoding modules of the variational auto-encoder is a very efficient way to constrain the generation to a given class, it requires the entire availability of labels for a given dataset. \n",
    "In his article [_Semi-supervised Learning with\n",
    "Deep Generative Models_](http://papers.nips.cc/paper/5352-semi-supervised-learning-with-deep-generative-models.pdf), Kingma & Rezende proposed an efficient way to perform _semi-supervised_ training with VAEs, using the label if available and inferring it if missing. This way, label information can be thought as _discrete latent variables_, modeled using multinomial distributions, that are sampled if the corresponding label is missing. In that case, the ELBO is calculated for every possible label and weighted average by the distribution $q(\\mathbf{y|x})$, performing closed-form expectation. In the other case, the true label is used as an input for the decoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, vschaos\n",
    "import vschaos.distributions as dist\n",
    "import vschaos.vaes as vaes\n",
    "from vschaos.data.data_generic import dataset_from_torchvision\n",
    "\n",
    "from torchvision.transforms import Lambda, ToTensor\n",
    "from vschaos.vaes import VanillaVAE\n",
    "\n",
    "from vschaos.criterions import SemiSupervisedELBO, Classification\n",
    "from vschaos.monitor import PCA\n",
    "from vschaos.train import SimpleTrainer, train_model\n",
    "\n",
    "# import MNIST\n",
    "transforms = [Lambda(lambda x: x / 255. + 0.01*torch.randn_like(x.float())), Flatten(-2)]\n",
    "dataset = dataset_from_torchvision('MNIST', transforms=transforms)\n",
    "\n",
    "# make VAE\n",
    "input_params = {'dim':784, 'dist': dist.Normal}\n",
    "class_params = {'dim':10, 'dist':dist.Categorical}\n",
    "# linked encoders and decoders should be used here to enforce the information sharing between categorical and latent\n",
    "hidden_params = {'dim':800, 'nlayers':2, 'batch_norm':'batch', 'linked':True}\n",
    "\n",
    "# we perform semi-supervision by adding a latent subdivision at first layer (don't forget the nested list, otherwise\n",
    "# the model will consider it as a second stochastic layer) and specifying the corresponding task\n",
    "latent_params = [[{'dim':8, \"dist\":dist.Normal}, \n",
    "                  {'dim':class_params['dim'], 'dist':dist.Multinomial, 'task':'class'}]]\n",
    "\n",
    "vae = vaes.VanillaVAE(input_params, latent_params, hidden_params=hidden_params)\n",
    "\n",
    "optim_params = {'optimizer':'Adam', 'optimArgs':{'lr':1e-5}, 'scheduler':'ReduceLROnPlateau'}\n",
    "vae.init_optimizer(optim_params)\n",
    "elbo = SemiSupervisedELBO(beta=1.0, warmup=20)\n",
    "\n",
    "# The Trainer object performs training, monitoring, and automating saving during the training process.\n",
    "plots = {}\n",
    "plots['reconstructions'] = {'preprocess': False, \"n_points\":15, 'plot_multihead':True, 'label':['class']}\n",
    "plots['latent_space'] = {'preprocess':False, 'transformation':PCA, 'tasks':'class', 'balanced':True, 'n_points':3000, 'label':['class'], 'batch_size':512}\n",
    "\n",
    "# the semi_supervision keyword has to be added, such that the trainer will alternate between the supervised\n",
    "#    and semi supervised case. \n",
    "trainer = SimpleTrainer(vae, dataset, elbo, tasks=[\"class\"], plots=plots, use_tensorboard=\"runs/\",\n",
    "                       semi_supervision=[\"class\"], semi_supervision_dropout = 0.2)\n",
    "\n",
    "train_options = {'epochs':100, 'save_epochs':20, 'results_folder':'tutorial_3',  'batch_size':64}\n",
    "train_model(trainer, train_options, save_with={'transforms':dataset.classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
